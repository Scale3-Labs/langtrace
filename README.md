# [Langtrace](https://www.langtrace.ai)

## Open Source & Open Telemetry(OTEL) Observability for LLM applications.

![Static Badge](https://img.shields.io/badge/License-AGPL--3.0-blue) ![Static Badge](https://img.shields.io/badge/npm_@langtrase/typescript--sdk-1.2.9-green) ![Static Badge](https://img.shields.io/badge/pip_langtrace--python--sdk-1.2.8-green) ![Static Badge](https://img.shields.io/badge/Development_status-Active-green)

---

Langtrace is an open source observability software which lets you capture, debug and analyze traces and metrics from all your applications that leverages LLM APIs, Vector Databases and LLM based Frameworks.

## Open Telemetry Support

The traces generated by Langtrace adhere to [Open Telemetry Standards(OTEL)](https://opentelemetry.io/docs/concepts/signals/traces/). We are developing [semantic conventions](https://opentelemetry.io/docs/concepts/semantic-conventions/) for the traces generated by this project. You can checkout the current definitions in [this repository](https://github.com/Scale3-Labs/langtrace-trace-attributes/tree/main/schemas). Note: This is an ongoing development and we encourage you to get involved and welcome your feedback.

---

## Getting Started

### Langtrace Cloud ☁️

To use the managed SaaS version of Langtrace, follow the steps below:

1. Sign up by going to [this link](www.langtrace.ai).
2. Create a new Project after signing up. Projects are containers for storing traces and metrics generated by your application. If you have only one application, creating 1 project will do.
3. Generate an API key by going inside the project.
4. In your application, install the Langtrace SDK and initialize it with the API key you generated in the step 3.
5. The code for installing and setting up the SDK is shown below:

### If your application is built using **typescript/javascript**:

``` typescript
npm i @langtrase/typescript-sdk
```
``` typescript
import * as Langtrace from '@langtrase/typescript-sdk // Must precede any llm module imports
Langtrace.init({ api_key: <your_api_key> })
```
OR
``` typescript
import * as Langtrace from '@langtrase/typescript-sdk // Must precede any llm module imports
LangTrace.init() // LANGTRACE_API_KEY as an ENVIRONMENT variable
```

### If your application is built using **python**:

```
pip install langtrace-python-sdk
```

```
from langtrace_python_sdk import langtrace
langtrace.init(api_key=process.env.LANGTRACE_API_KEY)
```

### Langtrace self hosted

To run the Langtrace locally, you have to run three services:

- Next.js app
- Postgres database
- Clickhouse database

Requirements:

- Docker
- Docker Compose

#### The .env file

Feel free to modify the `.env` file to suit your needs.

#### Starting the servers

```bash
docker compose up
```

The application will be available at `http://localhost:3000`.

> [!NOTE]  
> if you wish to build the docker image locally and use it, run the docker compose up command with the `--build` flag.

> [!TIP]
> to manually pull the docker image from docker hub, run the following command:
>
> ```bash
> docker pull scale3labs/langtrace-client:latest
> ```

#### Take down the setup

````bash

To delete containers and volumes

```bash
docker compose down -v
````

`-v` flag is used to delete volumes

#### Useful Ops Commands

The following are some commands that may come handy during setup and debugging.

<details>
  <summary>Connecting to postgres db</summary>
  
  ```bash
  docker exec -it langtrace-postgres psql --dbname=langtrace --username=ltuser --password
  ```

</details>

<details>
  <summary>Connecting to clickhouse server</summary>
  
  ```bash
  docker exec -it langtrace-clickhouse clickhouse-client
  ```
</details>

<details>
  <summary>Checking langtrace client app logs</summary>
  
  ```bash
  docker logs langtrace
  ```
  If you want to follow the logs
  ```bash
  docker logs -f langtrace
  ```
</details>

<details>
  <summary>Running prisma schema apply command</summary>
  
  ```bash
  docker exec -it langtrace npm run create-tables
  ```
</details>

#### Common issues for local setup

<details>
  <summary>Table not found error OR Column not found error</summary>
  Its likely that schema is not applied to the database or the schema is not in sync with the database. To fix this, run the following command:
  
  ```bash
  docker exec -it langtrace npm run create-tables
  ```
</details>

<details>
  <summary>Prisma schema not in sync with database</summary>
  If you have made changes to the prisma schema and want to apply the changes to the database, run the following command:
  
  ```bash
  docker exec -it langtrace npm run create-tables
  ```
</details>

<details>
  <summary>Docker compose failing to setup with `Additional property required is not allowed` errors</summary>
  Its likely that you are using an older version of docker-compose. Update docker-compose to the latest version.
  
  Certain docker compose schema used in this project are only supported in newer versions of docker-compose.

Either you **update the docker compose version** OR **remove the depends_on property** that is causing the error.

</details>

<details>
  <summary>Clickhouse server not starting</summary>
  If clickhouse server is not starting, it is likely that the port 8123 is already in use. You can change the port in the docker-compose file.
</details>

Install the langtrace SDK in your application by following the same instructions under the Langtrace Cloud section above for sending traces to your self hosted setup.

---

## SDK Repositories

- [Langtrace Typescript SDK](https://github.com/Scale3-Labs/langtrace-typescript-sdk)
- [Langtrace Python SDK](https://github.com/Scale3-Labs/langtrace-python-sdk)
- [Semantic Span Attributes](https://github.com/Scale3-Labs/langtrace-trace-attributes)

---

## Supported integrations

Langtrace automatically captures traces from the following vendors:

| Vendor       | Type            | Typescript SDK     | Python SDK         |
| ------------ | --------------- | ------------------ | ------------------ |
| OpenAI       | LLM             | :white_check_mark: | :white_check_mark: |
| Anthropic    | LLM             | :white_check_mark: | :white_check_mark: |
| Azure OpenAI | LLM             | :white_check_mark: | :white_check_mark: |
| Langchain    | Framework       | :x:                | :white_check_mark: |
| LlamaIndex   | Framework       | :white_check_mark: | :white_check_mark: |
| Pinecone     | Vector Database | :white_check_mark: | :white_check_mark: |
| ChromaDB     | Vector Database | :white_check_mark: | :white_check_mark: |

---

## Feature Requests and Issues

- To request for features, head over [here to start a discussion](https://github.com/Scale3-Labs/langtrace/discussions/categories/feature-requests).
- To raise an issue, head over [here and create an issue](https://github.com/Scale3-Labs/langtrace/issues).

---

## Contributions

We welcome contributions to this project. To get started, fork this repository and start developing. To get involved, join our Slack workspace.

---

## Security

To report security vulnerabilites, email us at security@scale3labs.com. You can read more on security [here](https://github.com/Scale3-Labs/langtrace/blob/development/SECURITY.md).

---

## License

- Langtrace application(this repository) is [licensed](https://github.com/Scale3-Labs/langtrace/blob/development/LICENSE) under the AGPL 3.0 License. You can read about this license [here](https://www.gnu.org/licenses/agpl-3.0.en.html).
- Langtrace SDKs are licensed under the Apache 2.0 License. You can read about this license [here](https://www.apache.org/licenses/LICENSE-2.0).

---

## Frequently Asked Questions

**1. Can I self host and run Langtrace in my own cloud?**
Yes, you can absolutely do that. Follow the self hosting setup instructions laid out above.

**2. What is the pricing for Langtrace cloud?**
Currently, we are not charging anything for Langtrace cloud and we are primarily looking for feedback so we can continue to improve the project. We will inform our users when we decide to monetize it.

**3. What is the tech stack of Langtrace?**
Langtrace uses NextJS for the frontend and APIs. It uses PostgresDB as a metadata store and Clickhouse DB for storing spans, metrics, logs and traces.

**4. Can I contribute to this project?**
Absolutely! We love developers and welcome contributions. Get involved early by joining our slack workspace.

**5. What skillset is required to contribute to this project?**
Programming Languages: Typescript and Python.
Framework knowledge: NextJS.
Database: Postgres and Prisma ORM.
Nice to haves: Opentelemetry instrumentation framework, experience with distributed tracing.
